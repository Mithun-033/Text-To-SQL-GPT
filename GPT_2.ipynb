{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiysBaHNj1A1RTejxJbocd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mithun-033/Text-To-SQL-GPT/blob/main/GPT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install galore_torch -q\n",
        "!pip install torchinfo -q"
      ],
      "metadata": {
        "id": "zY1p2e4RfCzO"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "import time\n",
        "from galore_torch import GaLoreAdamW\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "id": "d2NMDfJr2dPZ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev=\"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "Q2y97BTG5nWV"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    n_embed:int=1024\n",
        "    cwl:int=1024\n",
        "    b_size:int=32\n",
        "    n_head :int=16\n",
        "    head_size :int =1024//16\n",
        "    vocab_size :int=50304\n",
        "    n_layer :int=24\n"
      ],
      "metadata": {
        "id": "bWn4sO6E2vsR"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.config=config\n",
        "\n",
        "        self.q=nn.Linear(config.n_embed,config.head_size,bias=False)\n",
        "        self.k=nn.Linear(config.n_embed,config.head_size,bias=False)\n",
        "        self.v=nn.Linear(config.n_embed,config.head_size,bias=False)\n",
        "\n",
        "        self.dropout=nn.Dropout(p=0.1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        T=x.shape(1)\n",
        "\n",
        "        key=self.k(x)  # (B,T,H)\n",
        "        query=self.q(x)  # (B,T,H)\n",
        "        value=self.v(x)  # (B,T,H)\n",
        "\n",
        "        weights=nn.functional.scaled_dot_product_attention(query,key,value,is_causal=True)  #For faster training\n",
        "        # weights=(keys@(query.transpose(-2,-1)))/self.config.head_size**0.5 # (B,T,T)\n",
        "        # mask=torch.trill(torch.ones(T,T,device=dev))\n",
        "        # weights=weights.masked_fill(mask==0,float(\"-inf\"))\n",
        "\n",
        "        # weights=nn.Softmax(weights,dim=-1) #(B,T,T)\n",
        "\n",
        "        # logits=weights@value #(B,T,H)\n",
        "        logits=self.dropout(logits)\n",
        "        return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "Wm1Zp6jO4M16"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.config=config\n",
        "\n",
        "        self.Multi=nn.ModuleList([AttentionHead(config) for _ in range(config.n_head)])\n",
        "        self.project=nn.Linear(config.n_head*config.head_size,config.n_embed)\n",
        "        self.dropout=nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self,x):\n",
        "        output=torch.cat([head(x) for head in self.Multi],dim=-1) #(B,T,H*N)\n",
        "        output=self.project(output)#(B,T,C)\n",
        "        output=self.dropout(output)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "5XsIAW0T7T8o"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.config=config\n",
        "\n",
        "        self.w1=nn.Linear(config.n_embed,config.n_embed*4)\n",
        "        self.gate=nn.Linear(config.n_embed,config.n_embed*4,bias=False) #Removed the bias from the gate, i dont understand why i shld keep it\n",
        "        self.silu=nn.SiLU()\n",
        "\n",
        "        self.w2=nn.Linear(4*config.n_embed,config.n_embed)\n",
        "        self.dropout=nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out=self.silu(self.w1(x))*self.gate(x) #(B,T,5*C)\n",
        "        out=self.dropout(out)\n",
        "        out=self.w2(out) #(B,T,C)\n",
        "        return out"
      ],
      "metadata": {
        "id": "om94wUcV9Lio"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self,config,nth_block):\n",
        "        super().__init__()\n",
        "        self.config=config\n",
        "\n",
        "        self.PreNorm1=nn.RMSNorm(config.n_embed)\n",
        "        self.attention=MultiHeadAttention(config)\n",
        "        self.PreNorm2=nn.RMSNorm(config.n_embed)\n",
        "        self.FeedForwardLayer=MLP(config)\n",
        "\n",
        "        param=(nth_block+1)/config.n_layer\n",
        "        self.scaler=nn.Parameter(torch.tensor(param)) #I jus thought this might be gud, im makin a learnable paramter which decides how much\n",
        "        # the nth_block a transformer can affect the residual pathway\n",
        "        self.bias=nn.Parameter(torch.tensor(0.0)) #idk jus more degree of freedom ig\n",
        "    def forward(self,x):\n",
        "        x=x+self.attention(self.PreNorm1(x))*(self.scaler+self.bias)\n",
        "        x=x+self.FeedForwardLayer(self.PreNorm2(x))*(self.scaler+self.bias)  # Residual connection\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "8UpxUuDc_l8E"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "Z60PDLc-xp5Y"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.config=config\n",
        "\n",
        "        self.embed=nn.Embedding(config.vocab_size,config.n_embed)\n",
        "        self.pos_embed=nn.Embedding(config.cwl,config.n_embed)\n",
        "\n",
        "        self.blocks=nn.Sequential(\n",
        "            *[Block(config,i) for i in range(config.n_layer)]\n",
        "        )\n",
        "\n",
        "        self.final_norm=nn.RMSNorm(config.n_embed)\n",
        "        self.Dense=nn.Linear(config.n_embed,config.vocab_size,bias=False)\n",
        "\n",
        "        self.Dense.weight=self.embed.weight\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"pos_ids\",\n",
        "            torch.arange(config.cwl),\n",
        "            persistent=False  #This makes it so tht it wont save this thing when i do model.state_dict()\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        B,T=x.shape\n",
        "\n",
        "        tok=self.embed(x)              # (B,T,C)\n",
        "        pos=self.pos_embed(self.pos_ids[:T])    # (T,C)\n",
        "\n",
        "        x=tok+pos #sneaky pytorch broadcast\n",
        "\n",
        "        x=self.blocks(x)\n",
        "        x=self.final_norm(x)\n",
        "        logits=self.Dense(x)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generator(ids,b_size,cwl):\n",
        "    step=b_size*cwl\n",
        "    for i in range(0,len(ids)-step-1,step):\n",
        "        Tot=torch.from_numpy(ids[i:i+step+1])\n",
        "        X=Tot[:-1].view(b_size,cwl)\n",
        "        y=Tot[1:].view(b_size,cwl)\n",
        "        yield X,y\n"
      ],
      "metadata": {
        "id": "FJNvLqytbT6-"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=GPT(Config())\n",
        "model=torch.compile(model)\n",
        "model.to(dev)\n",
        "\n",
        "summary(model)"
      ],
      "metadata": {
        "id": "bsRZCHDZMOXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33449c95-756a-4ba6-da62-6b48a8a44b2e"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "================================================================================\n",
              "Layer (type:depth-idx)                                  Param #\n",
              "================================================================================\n",
              "OptimizedModule                                         --\n",
              "├─GPT: 1-1                                              --\n",
              "│    └─Embedding: 2-1                                   51,511,296\n",
              "│    └─Embedding: 2-2                                   1,048,576\n",
              "│    └─Sequential: 2-3                                  --\n",
              "│    │    └─Block: 3-1                                  16,785,409\n",
              "│    │    └─Block: 3-2                                  16,785,409\n",
              "│    │    └─Block: 3-3                                  16,785,409\n",
              "│    │    └─Block: 3-4                                  16,785,409\n",
              "│    │    └─Block: 3-5                                  16,785,409\n",
              "│    │    └─Block: 3-6                                  16,785,409\n",
              "│    │    └─Block: 3-7                                  16,785,409\n",
              "│    │    └─Block: 3-8                                  16,785,409\n",
              "│    │    └─Block: 3-9                                  16,785,409\n",
              "│    │    └─Block: 3-10                                 16,785,409\n",
              "│    │    └─Block: 3-11                                 16,785,409\n",
              "│    │    └─Block: 3-12                                 16,785,409\n",
              "│    │    └─Block: 3-13                                 16,785,409\n",
              "│    │    └─Block: 3-14                                 16,785,409\n",
              "│    │    └─Block: 3-15                                 16,785,409\n",
              "│    │    └─Block: 3-16                                 16,785,409\n",
              "│    │    └─Block: 3-17                                 16,785,409\n",
              "│    │    └─Block: 3-18                                 16,785,409\n",
              "│    │    └─Block: 3-19                                 16,785,409\n",
              "│    │    └─Block: 3-20                                 16,785,409\n",
              "│    │    └─Block: 3-21                                 16,785,409\n",
              "│    │    └─Block: 3-22                                 16,785,409\n",
              "│    │    └─Block: 3-23                                 16,785,409\n",
              "│    │    └─Block: 3-24                                 16,785,409\n",
              "│    └─RMSNorm: 2-4                                     1,024\n",
              "│    └─Linear: 2-5                                      51,511,296\n",
              "================================================================================\n",
              "Total params: 506,922,008\n",
              "Trainable params: 506,922,008\n",
              "Non-trainable params: 0\n",
              "================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_batches=100000000/(Config.b_size*Config.cwl)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "epochs=1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5mVlXF-aWOU",
        "outputId": "c2a721b7-575e-46e7-8f85-45130651db32"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/galore_torch/adamw.py:48: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "galore_params=[]\n",
        "normal_params=[]\n",
        "for j,i in model.named_parameters():\n",
        "    if i.ndim<2:\n",
        "        normal_params.append(i)\n",
        "    else:\n",
        "        galore_params.append(i)\n",
        "\n",
        "optimizer=GaLoreAdamW([\n",
        "    {\"params\":galore_params,\"rank\":128},\n",
        "    {\"params\":normal_params}\n",
        "    ],\n",
        "    lr=6e-4)\n",
        "\n",
        "warmup_steps=20\n",
        "cosine_steps=total_batches-warmup_steps\n",
        "warmup_scheduler=torch.optim.lr_scheduler.LinearLR(optimizer=optimizer,\n",
        "                                                   start_factor=0.1,\n",
        "                                                   end_factor=1,\n",
        "                                                   total_iters=warmup_steps)\n",
        "cosine_scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer,\n",
        "                                                            T_max=cosine_steps)\n",
        "\n",
        "scheduler=torch.optim.lr_scheduler.SequentialLR(optimizer=optimizer,\n",
        "                                                schedulers=[warmup_scheduler,cosine_scheduler],\n",
        "                                                milestones=warmup_steps)\n"
      ],
      "metadata": {
        "id": "SBvf_x3PnY6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(11):\n",
        "    ids=np.load(f\"tokens_{k}.npy\",mmap_mode=\"r\")\n",
        "    for i in range(epochs):\n",
        "        start=time.time()\n",
        "        loss_accum=0\n",
        "        optimizer.zero_grad()\n",
        "        for x,y in generator(ids,Config.b_size,Config.cwl):\n",
        "            model.train()\n",
        "            with torch.autocast(device_type=dev,dtype=torch.bfloat16):\n",
        "                out=model(x)\n",
        "                out=out.view(-1,Config.vocab_size)\n",
        "                y=y.view(-1)\n",
        "                loss=criterion(out,y)\n",
        "                loss_accum+=loss.item()\n",
        "                loss=loss/4\n",
        "                loss.backward()\n",
        "\n",
        "                steps+=1\n",
        "\n",
        "                if steps%4==0:\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                if steps%100==0:\n",
        "                    end=time.time()\n",
        "                    print(f\"Loss :{loss_accum/100:.5f},Time :{end-start},Batches :{steps/total_batches}\")\n",
        "                    torch.save({\n",
        "                        \"model\":model.state_dict(),\n",
        "                        \"optimizer\":optimizer.state_dict(),\n",
        "                        \"scheduler\":scheduler.state_dict(),\n",
        "                        \"step\":steps\n",
        "                    },\"GPT-2.pt\")\n",
        "\n",
        "                    start=end\n",
        "                    loss_accum=0\n",
        "\n",
        "                if steps%1000==0:\n",
        "                    model.eval()\n",
        "                    loss_accum_val=0\n",
        "                    val_steps=0\n",
        "                    with torch.no_grad():\n",
        "                        for x,y in generator(val_ids,Config.b_size,Config.cwl):\n",
        "                            with torch.autocast(device_type=dev,dtype=torch.bfloat16):\n",
        "                                out=model(x)\n",
        "                                out=out.view(-1,Config.vocab_size)\n",
        "                                y=y.view(-1)\n",
        "                                loss=criterion(out,y)\n",
        "                            loss_accum_val+=loss.item()\n",
        "                            val_steps+=1\n",
        "\n",
        "\n",
        "                    print(f\"Val Loss :{loss_accum_val/val_steps:.4f}\")"
      ],
      "metadata": {
        "id": "xz5Ca8b0xyYu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}