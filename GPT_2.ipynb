{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNS++MRrG2Fs/WUcggiPWO2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mithun-033/Text-To-SQL-GPT/blob/main/GPT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "import time"
      ],
      "metadata": {
        "id": "d2NMDfJr2dPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev=\"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "Q2y97BTG5nWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    n_embed:int=1\n",
        "    cwl:int=1\n",
        "    b_size:int=1\n",
        "    head_size :int=1\n",
        "    n_head :int=1\n",
        "    vocab_size :int=1\n",
        "    n_layer :int=1\n",
        ""
      ],
      "metadata": {
        "id": "bWn4sO6E2vsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.config=config\n",
        "\n",
        "        self.q=nn.Linear(config.n_embed,config.head_size,bias=False)\n",
        "        self.k=nn.Linear(config.n_embed,config.head_size,bias=False)\n",
        "        self.v=nn.Linear(config.n_embed,config.head_size,bias=False)\n",
        "\n",
        "        self.dropout=nn.Dropout(p=0.15)\n",
        "\n",
        "    def forward(self,x):\n",
        "        T=x.shape(1)\n",
        "\n",
        "        keys=self.k(x)  # (B,T,H)\n",
        "        query=self.q(x)  # (B,T,H)\n",
        "        value=self.v(x)  # (B,T,H)\n",
        "\n",
        "        weights=(keys@(query.transpose(-2,-1)))/self.config.head_size**0.5 # (B,T,T)\n",
        "        mask=torch.trill(torch.ones(T,T,device=dev))\n",
        "        weights=weights.masked_fill(mask==0,float(\"-inf\"))\n",
        "\n",
        "        weights=nn.Softmax(weights,dim=-1) #(B,T,T)\n",
        "\n",
        "        logits=weights@value #(B,T,H)\n",
        "        logits=self.dropout(logits)\n",
        "        return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "Wm1Zp6jO4M16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.config=config\n",
        "\n",
        "        self.Multi=nn.ModuleList([AttentionHead() for _ in range(config.n_head)])\n",
        "        self.project=nn.Linear(config.n_head*config.head_size,config.n_embed)\n",
        "        self.dropout=nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self,x):\n",
        "        output=torch.cat([head(x) for head in self.Multi],dim=-1) #(B,T,H*N)\n",
        "        output=self.project(output) #(B,T,N)\n",
        "        output=self.dropout(output)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "5XsIAW0T7T8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.config=config\n",
        "\n",
        "        self.layer=nn.Sequential(\n",
        "            nn.Linear(config.n_embed,5*config.n_embed),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(5*config.n_embed,config.n_embed)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.layer(x)"
      ],
      "metadata": {
        "id": "om94wUcV9Lio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.config=config\n",
        "\n",
        "        self.PreNorm1=nn.LayerNorm(config.n_embed)\n",
        "        self.attention=MultiHeadAttention()\n",
        "        self.PreNorm2=nn.LayerNorm(config.n_embed)\n",
        "        self.FeedForwardLayer=MLP()\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=x+self.attention(self.PreNorm1(x))\n",
        "        x=x+self.FeedForwardLayer(self.PreNorm2)  # Residual connection\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "8UpxUuDc_l8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z60PDLc-xp5Y"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.config=config\n",
        "        # X --> (B,T)\n",
        "        self.embed = nn.Embedding(config.vocab_size,config.n_embed)  # (B,T,C)\n",
        "        self.pos_embed=nn.Linear(config.cwl,config.n_embed) # (T,C)\n",
        "        self.blocks=nn.Sequential(*[Block() for _ in range(config.n_layer)])\n",
        "        self.Final_norm=nn.LayerNorm(config.n_embed)\n",
        "        self.Dense=nn.Linear(config.n_embed,config.vocab_size)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.embed(x) # (B,T,C)\n",
        "        pos=self.pos_embed(torch.arange(self.config.cwl),device=dev)\n",
        "        x+=pos #sneaky broadcast\n",
        "\n",
        "        out=self.blocks(x) # (B,T,C)\n",
        "        out=self.Final_norm(out) # (B,T,C)\n",
        "\n",
        "        logits=self.Dense(out) #(B,T,V)\n",
        "\n",
        "        return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=GPT()\n",
        "model=torch.compile(model)\n",
        "model.to(device)\n",
        "optimizer=torch.optim.AdamW(model.parameters(),lr=6e-4)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "epochs=1\n",
        "total_batches=len(ids)/b_size"
      ],
      "metadata": {
        "id": "bsRZCHDZMOXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    steps=0\n",
        "    start=time.time()\n",
        "    loss_accum=0\n",
        "    for x,y in generator(ids,batch_size,cwl):\n",
        "        with torch.autocast(device_type=dev,dtype=torch.bfloat16):\n",
        "            out=model(x)\n",
        "            out=out.view(-1,1)\n",
        "            y=y.view(-1)\n",
        "            loss+=criterion(out,y)/4\n",
        "            loss_accum+=loss.item()\n",
        "            loss.backward()\n",
        "\n",
        "            steps+=1\n",
        "\n",
        "            if steps%4==0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            if steps%100:\n",
        "                end=time.time()\n",
        "                print(f\"Loss :{loss_accum},Time :{start-end},Batches :{steps/total_batches}\")\n",
        "                start=end\n",
        "                loss_accum=0\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xz5Ca8b0xyYu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}